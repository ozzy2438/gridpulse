# GridPulse - Energy Integration Platform

## Overview

GridPulse is a reference architecture demonstrating enterprise-grade integration patterns for energy market data using webMethods, Kafka, and Kong.

### Problem Statement

Large energy companies face these challenges:
- **Data Inconsistency**: Multiple teams fetch same data differently
- **Integration Sprawl**: Point-to-point connections grow exponentially
- **No Traceability**: "Why was this data delayed?" has no answer
- **Slow Onboarding**: Adding new consumers takes weeks

### Solution

```
Data Sources â†’ webMethods (normalize) â†’ Kafka (event hub) â†’ Kong (API gateway) â†’ Consumers
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           GridPulse Architecture                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚  AEMO API    â”‚    â”‚ Weather API  â”‚    DATA SOURCES                       â”‚
â”‚  â”‚  (Dispatch)  â”‚    â”‚ (Open-Meteo) â”‚                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚         â”‚                   â”‚                                                â”‚
â”‚         â–¼                   â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚     webMethods / Python Scripts     â”‚    INGESTION LAYER                 â”‚
â”‚  â”‚  - Fetch & Poll data sources        â”‚    - Normalize to canonical model  â”‚
â”‚  â”‚  - Transform & Validate             â”‚    - Add correlation IDs           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                 â”‚                                                            â”‚
â”‚                 â–¼                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚         Apache Kafka                â”‚    EVENT HUB                       â”‚
â”‚  â”‚  Topics:                            â”‚    - Decouple producers/consumers  â”‚
â”‚  â”‚  - market.dispatch (3 partitions)   â”‚    - Replay capability             â”‚
â”‚  â”‚  - weather.observations             â”‚    - 7-day retention               â”‚
â”‚  â”‚  - dlq.* (Dead Letter Queues)       â”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                 â”‚                                                            â”‚
â”‚                 â–¼                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚         Kong API Gateway            â”‚    API LAYER                       â”‚
â”‚  â”‚  - Authentication (API Keys)        â”‚    - Rate limiting                 â”‚
â”‚  â”‚  - Correlation ID injection         â”‚    - Logging & metrics             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                 â”‚                                                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚         â–¼       â–¼       â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚Analytics â”‚ â”‚Operationsâ”‚ â”‚  Risk    â”‚    CONSUMERS                        â”‚
â”‚  â”‚  Team    â”‚ â”‚  Team    â”‚ â”‚  Team    â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.9+
- curl (for API testing)

### Automated Setup (Recommended)

```bash
# Clone the repo
cd gridpulse

# Run the quick start script
./start.sh
```

This script will:
- âœ… Check Docker is running
- âœ… Start all services (Kafka, Kong, Monitoring)
- âœ… Create Kafka topics
- âœ… Configure Kong API Gateway
- âœ… Set up Python virtual environment
- âœ… Test data sources (real weather data!)

### Manual Setup

#### 1. Start Infrastructure

```bash
# Make sure Docker Desktop is running
# macOS: open -a Docker

# Start all services
docker compose up -d

# Wait for services to be healthy (about 60 seconds)
docker compose ps
```

#### 2. Create Kafka Topics

```bash
chmod +x scripts/create_kafka_topics.sh
./scripts/create_kafka_topics.sh
```

#### 3. Configure Kong

```bash
chmod +x scripts/setup_kong.sh
./scripts/setup_kong.sh
```

#### 4. Set Up Python Environment

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 5. Start API Server

```bash
# In terminal 1
source venv/bin/activate
python scripts/api_server.py
```

#### 6. Run Data Pipeline

```bash
# In terminal 2 (fetches REAL data from APIs!)
source venv/bin/activate
python scripts/data_pipeline.py

# Or run continuously every 5 minutes
python scripts/data_pipeline.py --continuous --interval 300
```

#### 7. Test the API

```bash
# Without API key (should fail with 401)
curl http://localhost:8100/v1/market/dispatch

# With API key (should succeed)
curl -H "apikey: analytics-team-secret-key-2024" \
  http://localhost:8100/v1/market/dispatch
```

## Access Points

| Service | URL | Credentials |
|---------|-----|-------------|
| Kafka UI | http://localhost:8180 | - |
| Kong Admin | http://localhost:8101 | - |
| Kong Proxy | http://localhost:8100 | API Key required |
| Kong Manager | http://localhost:8102 | - |
| Grafana | http://localhost:3001 | admin/admin |
| Prometheus | http://localhost:9090 | - |
| API Server | http://localhost:5001 | - |

## API Keys

| Team | API Key |
|------|---------|
| Analytics | `analytics-team-secret-key-2024` |
| Operations | `ops-team-secret-key-2024` |
| Risk | `risk-team-secret-key-2024` |

## API Endpoints

### Health Check
```bash
GET /health
```

### Market Dispatch
```bash
# Get dispatch events
GET /api/v1/dispatch?region_id=NSW1&limit=100

# Add dispatch event
POST /api/v1/dispatch
Content-Type: application/json
{
  "region_id": "NSW1",
  "fuel_type": "solar",
  "value_mw": 1250.5
}
```

### Weather Observations
```bash
GET /api/v1/weather?region_id=NSW1&limit=100
```

### Statistics
```bash
GET /api/v1/stats
```

## Project Structure

```
gridpulse/
â”œâ”€â”€ start.sh                    # ğŸš€ Quick start script (run this!)
â”œâ”€â”€ docker-compose.yml          # All services definition
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ kafka/                  # Kafka configuration
â”‚   â””â”€â”€ kong/
â”‚       â””â”€â”€ kong.yml            # Kong declarative config
â”œâ”€â”€ webmethods/
â”‚   â”œâ”€â”€ services/               # webMethods services (conceptual)
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ MarketDispatchEvent.xsd
â”‚       â””â”€â”€ WeatherObservation.xsd
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_pipeline.py        # ğŸŒŸ Complete data pipeline (real APIs!)
â”‚   â”œâ”€â”€ download_aemo.py        # AEMO data fetcher
â”‚   â”œâ”€â”€ test_data_fetch.py      # Test data sources without Kafka
â”‚   â”œâ”€â”€ kafka_producer.py       # Kafka producer
â”‚   â”œâ”€â”€ api_server.py           # Flask API server
â”‚   â”œâ”€â”€ create_kafka_topics.sh  # Topic creation script
â”‚   â””â”€â”€ setup_kong.sh           # Kong configuration script
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw data from sources (real weather data!)
â”‚   â””â”€â”€ processed/              # Processed data
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ prometheus.yml          # Prometheus configuration
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ adr/                    # Architecture Decision Records
â””â”€â”€ tests/                      # Test files
```

## Key Concepts

### Canonical Model
All data is normalized to a standard format before entering the system. This ensures consistency across all consumers.

### Correlation ID
Every request gets a unique correlation ID that flows through the entire system, enabling end-to-end traceability.

### Dead Letter Queue (DLQ)
Failed messages are sent to DLQ topics for later analysis and retry, ensuring zero data loss.

### Idempotency
Event IDs are generated deterministically, allowing consumers to deduplicate messages and achieve exactly-once processing.

## Demo Scenarios

### Scenario 1: Add New Consumer

```bash
# Create new consumer in Kong (takes < 5 minutes)
curl -X POST http://localhost:8001/consumers \
  --data username=new-team

curl -X POST http://localhost:8001/consumers/new-team/key-auth \
  --data key=new-team-secret-key-2024

# Test immediately
curl -H "apikey: new-team-secret-key-2024" \
  http://localhost:8000/v1/market/dispatch
```

### Scenario 2: Trace a Request

```bash
# Make a request and capture correlation ID
curl -i -H "apikey: analytics-team-secret-key-2024" \
  http://localhost:8000/v1/market/dispatch

# Look for: X-Correlation-ID header in response
# Use this ID to search logs across all systems
```

### Scenario 3: Rate Limiting

```bash
# Send many requests quickly
for i in {1..110}; do
  curl -s -o /dev/null -w "%{http_code}\n" \
    -H "apikey: analytics-team-secret-key-2024" \
    http://localhost:8000/v1/market/dispatch
done | sort | uniq -c

# After 100 requests, you'll see 429 (Too Many Requests)
```

## Stopping Services

```bash
# Stop all services
docker-compose down

# Stop and remove volumes (clean slate)
docker-compose down -v
```

## Troubleshooting

### Kafka not starting
```bash
# Check Zookeeper first
docker logs gridpulse-zookeeper

# Then check Kafka
docker logs gridpulse-kafka
```

### Kong returning 502
```bash
# Ensure API server is running
python scripts/api_server.py

# Check Kong upstream health
curl http://localhost:8001/upstreams
```

### API returns empty data
```bash
# Send some test data first
python scripts/kafka_producer.py
```

## License

MIT License
